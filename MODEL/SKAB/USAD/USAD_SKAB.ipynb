{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gioel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\gioel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\Users\\gioel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files=[]\n",
    "file_name=[]\n",
    "import os\n",
    "for root, dirs, files in os.walk(\"../data/\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "             all_files.append(os.path.join(root, file))\n",
    "             file_name.append(f'{root}/{file}')\n",
    "# datasets with anomalies loading\n",
    "list_of_df = [pd.read_csv(file, \n",
    "                          sep=';', \n",
    "                          index_col='datetime', \n",
    "                          parse_dates=True) for file in all_files if 'anomaly-free' not in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils import *\n",
    "device = get_default_device()\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self, in_size, latent_size):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.linear1 = nn.Linear(in_size, int(in_size/2))\n",
    "    self.linear2 = nn.Linear(int(in_size/2), int(in_size/4))\n",
    "    self.linear3 = nn.Linear(int(in_size/4), latent_size)\n",
    "    self.relu = nn.ReLU(True)\n",
    "        \n",
    "  def forward(self, w):\n",
    "    out = self.linear1(w)\n",
    "    out = self.relu(out)\n",
    "    out = self.linear2(out)\n",
    "    out = self.relu(out)\n",
    "    out = self.linear3(out)\n",
    "    z = self.relu(out)\n",
    "\n",
    "    return z\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, latent_size, out_size):\n",
    "    super().__init__()\n",
    "\n",
    "    self.linear1 = nn.Linear(latent_size, int(out_size/4))\n",
    "    self.linear2 = nn.Linear(int(out_size/4), int(out_size/2))\n",
    "    self.linear3 = nn.Linear(int(out_size/2), out_size)\n",
    "    self.relu = nn.ReLU(True)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "  def forward(self, z):\n",
    "\n",
    "    out = self.linear1(z)\n",
    "    out = self.relu(out)\n",
    "    out = self.linear2(out)\n",
    "    out = self.relu(out)\n",
    "    out = self.linear3(out)\n",
    "    w = self.sigmoid(out)\n",
    "    return w\n",
    "    \n",
    "class UsadModel(nn.Module):\n",
    "  def __init__(self, w_size, z_size):\n",
    "    super().__init__()\n",
    "    self.encoder = Encoder(w_size, z_size)\n",
    "    self.decoder1 = Decoder(z_size, w_size)\n",
    "    self.decoder2 = Decoder(z_size, w_size)\n",
    "  \n",
    "  def training_step(self, batch, n):\n",
    "    z = self.encoder(batch)\n",
    "    w1 = self.decoder1(z)\n",
    "    w2 = self.decoder2(z)\n",
    "    w3 = self.decoder2(self.encoder(w1))\n",
    "    loss1 = 1/n*torch.mean((batch-w1)**2)+(1-1/n)*torch.mean((batch-w3)**2)\n",
    "    loss2 = 1/n*torch.mean((batch-w2)**2)-(1-1/n)*torch.mean((batch-w3)**2)\n",
    "    return loss1,loss2\n",
    "\n",
    "  def validation_step(self, batch, n):\n",
    "    z = self.encoder(batch)\n",
    "    w1 = self.decoder1(z)\n",
    "    w2 = self.decoder2(z)\n",
    "    w3 = self.decoder2(self.encoder(w1))\n",
    "    loss1 = 1/n*torch.mean((batch-w1)**2)+(1-1/n)*torch.mean((batch-w3)**2)\n",
    "    loss2 = 1/n*torch.mean((batch-w2)**2)-(1-1/n)*torch.mean((batch-w3)**2)\n",
    "    return {'val_loss1': loss1, 'val_loss2': loss2}\n",
    "        \n",
    "  def validation_epoch_end(self, outputs):\n",
    "    batch_losses1 = [x['val_loss1'] for x in outputs]\n",
    "    epoch_loss1 = torch.stack(batch_losses1).mean()\n",
    "    batch_losses2 = [x['val_loss2'] for x in outputs]\n",
    "    epoch_loss2 = torch.stack(batch_losses2).mean()\n",
    "    return {'val_loss1': epoch_loss1.item(), 'val_loss2': epoch_loss2.item()}\n",
    "    \n",
    "  def epoch_end(self, epoch, result):\n",
    "    #print(\"Epoch [{}], val_loss1: {:.4f}, val_loss2: {:.4f}\".format(epoch, result['val_loss1'], result['val_loss2']))\n",
    "    pollo=0\n",
    "    \n",
    "def evaluate(model, val_loader, n):\n",
    "    outputs = [model.validation_step(to_device(batch,device), n) for [batch] in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def training(epochs, model, train_loader, val_loader, opt_func=torch.optim.Adam):\n",
    "    history = []\n",
    "\n",
    "\n",
    "    optimizer1 = opt_func(list(model.encoder.parameters())+list(model.decoder1.parameters()))\n",
    "    optimizer2 = opt_func(list(model.encoder.parameters())+list(model.decoder2.parameters()))\n",
    "    for epoch in range(epochs):\n",
    "        for [batch] in train_loader:\n",
    "            batch=to_device(batch,device)\n",
    "            \n",
    "            #Train AE1\n",
    "            loss1,loss2 = model.training_step(batch,epoch+1)\n",
    "            loss1.backward()\n",
    "            optimizer1.step()\n",
    "            optimizer1.zero_grad()\n",
    "            \n",
    "            \n",
    "            #Train AE2\n",
    "            loss1,loss2 = model.training_step(batch,epoch+1)\n",
    "            loss2.backward()\n",
    "            optimizer2.step()\n",
    "            optimizer2.zero_grad()\n",
    "            \n",
    "            \n",
    "        result = evaluate(model, val_loader, epoch+1)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history\n",
    "    \n",
    "def testing(model, test_loader, alpha=.5, beta=.5):\n",
    "    results=[]\n",
    "    for [batch] in test_loader:\n",
    "        batch=to_device(batch,device)\n",
    "        w1=model.decoder1(model.encoder(batch))\n",
    "        w2=model.decoder2(model.encoder(w1))\n",
    "        results.append(alpha*torch.mean((batch-w1)**2,axis=1)+beta*torch.mean((batch-w2)**2,axis=1))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redefine Window 3-6-12-24-48\n",
    "\n",
    "WINDOW_SIZE =48\n",
    "def create_sequences(values, time_steps=WINDOW_SIZE):\n",
    "    output = []\n",
    "    for i in range(len(values) - time_steps):\n",
    "        output.append(values[i : (i + time_steps)])\n",
    "    return np.stack(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [06:00<00:00, 10.93s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import torch.utils.data as data_utils\n",
    "from tqdm import tqdm\n",
    "errore=[]\n",
    "PREDICTION =[]\n",
    "\n",
    "BATCH_SIZE =  64\n",
    "N_EPOCHS = 100\n",
    "hidden_size = 8\n",
    "\n",
    "iter=1\n",
    "for df in tqdm(list_of_df):\n",
    "\n",
    "            anomaly=df['anomaly']\n",
    "            X_train = df[:400].drop(['anomaly','changepoint'], axis=1)\n",
    "            X_test = df.drop(['anomaly','changepoint'], axis=1)\n",
    "            X_val = df[:400].drop(['anomaly','changepoint'], axis=1)\n",
    "            #X_val = df[:450].drop(['anomaly','changepoint'], axis=1)\n",
    "\n",
    "\n",
    "            scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled  = scaler.transform(X_test)\n",
    "            X_val_scaled  = scaler.transform(X_val)\n",
    "\n",
    "            \n",
    "            X_train_scaled=create_sequences(X_train_scaled,WINDOW_SIZE)\n",
    "            X_test_scaled=create_sequences(X_test_scaled,WINDOW_SIZE)\n",
    "            X_val_scaled=create_sequences(X_val_scaled,WINDOW_SIZE)\n",
    "\n",
    "            w_size=X_train_scaled.shape[1]*X_train_scaled.shape[2]\n",
    "            z_size=hidden_size\n",
    "\n",
    "            train_loader = torch.utils.data.DataLoader(data_utils.TensorDataset(\n",
    "                    torch.from_numpy(X_train_scaled).float().reshape(([X_train_scaled.shape[0],w_size]))\n",
    "                ) , batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "            val_loader = torch.utils.data.DataLoader(data_utils.TensorDataset(\n",
    "                    torch.from_numpy(X_val_scaled).float().reshape(([X_val_scaled.shape[0],w_size]))\n",
    "                ) , batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "            test_loader = torch.utils.data.DataLoader(data_utils.TensorDataset(\n",
    "                    torch.from_numpy(X_test_scaled).float().reshape(([X_test_scaled.shape[0],w_size]))\n",
    "                ) , batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "            model = UsadModel(w_size, z_size)\n",
    "            model = to_device(model,device)\n",
    "\n",
    "            #SAVE MODEL\n",
    "            #history = training(N_EPOCHS,model,train_loader,val_loader)\n",
    "            #torch.save(model.state_dict(), f'./../MODEL_Saved/USAD/MODEL{WINDOW_SIZE}/model{iter}'')\n",
    "            \n",
    "            #LOAD MODEL\n",
    "            model = UsadModel(w_size, z_size)\n",
    "            model.load_state_dict(torch.load(f'./../MODEL_Saved/USAD/MODEL{WINDOW_SIZE}/model{iter}'))\n",
    "            model.eval()\n",
    "\n",
    "            results=testing(model,test_loader)\n",
    "            y_pred=np.concatenate([torch.stack(results[:-1]).flatten().detach().cpu().numpy(),\n",
    "                                results[-1].flatten().detach().cpu().numpy()])\n",
    "            \n",
    "            results=testing(model,val_loader)\n",
    "            y_val=np.concatenate([torch.stack(results[:-1]).flatten().detach().cpu().numpy(),\n",
    "                                results[-1].flatten().detach().cpu().numpy()])\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "            th=np.max(y_val)\n",
    "\n",
    "            outlier=y_pred>th\n",
    "            \n",
    "            predi=[False for j in range(len(anomaly))]\n",
    "            for i in range(len(y_pred)):\n",
    "                predi[i+WINDOW_SIZE-1]=outlier[i]\n",
    "\n",
    "            PREDICTION.append(pd.Series(np.array(predi).astype(int), index=df.index))\n",
    "            iter=iter+1\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total value: 36621\n",
      "TRUE POSITIVE: 12002\n",
      "TRUE NEGATIVE: 17141\n",
      "FALSE POSITIVE: 6797\n",
      "FALSE NEGATIVE: 681\n",
      "PRECISION: 0.6384382147986595\n",
      "RECALL: 0.9463060790033904\n",
      "ACCURANCY: 0.7958002239152399\n",
      "False Alarm Rate 28.39 %\n",
      "Missing Alarm Rate 5.37 %\n",
      "F1 metric 0.7624674417127246\n"
     ]
    }
   ],
   "source": [
    "import metrics as metric\n",
    "#EVALUATION METRICS\n",
    "true_outlier = [df.anomaly for df in list_of_df]\n",
    "metric.evalu(true_outlier,PREDICTION, metric='binary', window_time='30 sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:03<00:00, 10.59it/s]\n",
      "100%|██████████| 33/33 [00:02<00:00, 11.18it/s]\n",
      "100%|██████████| 33/33 [00:02<00:00, 12.23it/s]\n",
      "100%|██████████| 33/33 [00:02<00:00, 12.17it/s]\n",
      "100%|██████████| 33/33 [00:02<00:00, 12.03it/s]\n",
      "100%|██████████| 33/33 [00:02<00:00, 11.35it/s]\n",
      "100%|██████████| 33/33 [00:02<00:00, 13.21it/s]\n",
      "100%|██████████| 33/33 [00:02<00:00, 11.21it/s]\n",
      "100%|██████████| 33/33 [00:02<00:00, 11.10it/s]\n",
      "100%|██████████| 33/33 [00:02<00:00, 11.68it/s]\n",
      "100%|██████████| 33/33 [00:02<00:00, 12.16it/s]\n",
      "100%|██████████| 33/33 [00:02<00:00, 11.67it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import torch.utils.data as data_utils\n",
    "from tqdm import tqdm\n",
    "\n",
    "BATCH_SIZE =  64\n",
    "N_EPOCHS = 200\n",
    "hidden_size = 8\n",
    "\n",
    "\n",
    "import csv\n",
    "header=['th_factor','method','F1','precision','recall','TP','TN','FP','FN']\n",
    "\n",
    "mt=['IQR','MAD','STD']\n",
    "\n",
    "with open('USAD_.csv', 'w', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    writer.writerow(header)\n",
    "    thresholding_factor=[0.5,1,1.5,2]\n",
    "    for k in range(3):\n",
    "        for t in thresholding_factor:\n",
    "            iter=1\n",
    "            TP=0\n",
    "            TN=0\n",
    "            FN=0\n",
    "            FP=0\n",
    "            for df in tqdm(list_of_df):\n",
    "\n",
    "                        anomaly=df['anomaly']\n",
    "                        X_train = df[:400].drop(['anomaly','changepoint'], axis=1)\n",
    "                        X_test = df.drop(['anomaly','changepoint'], axis=1)\n",
    "                        X_val = df[:450].drop(['anomaly','changepoint'], axis=1)\n",
    "                        \n",
    "\n",
    "\n",
    "                        scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "                        X_train_scaled = scaler.fit_transform(X_train)\n",
    "                        X_test_scaled  = scaler.transform(X_test)\n",
    "                        X_val_scaled  = scaler.transform(X_val)\n",
    "\n",
    "                        \n",
    "                        X_train_scaled=create_sequences(X_train_scaled,WINDOW_SIZE)\n",
    "                        X_test_scaled=create_sequences(X_test_scaled,WINDOW_SIZE)\n",
    "                        X_val_scaled=create_sequences(X_val_scaled,WINDOW_SIZE)\n",
    "\n",
    "                        w_size=X_train_scaled.shape[1]*X_train_scaled.shape[2]\n",
    "                        z_size=hidden_size\n",
    "\n",
    "                        train_loader = torch.utils.data.DataLoader(data_utils.TensorDataset(\n",
    "                                torch.from_numpy(X_train_scaled).float().reshape(([X_train_scaled.shape[0],w_size]))\n",
    "                            ) , batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "                        val_loader = torch.utils.data.DataLoader(data_utils.TensorDataset(\n",
    "                                torch.from_numpy(X_val_scaled).float().reshape(([X_val_scaled.shape[0],w_size]))\n",
    "                            ) , batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "                        test_loader = torch.utils.data.DataLoader(data_utils.TensorDataset(\n",
    "                                torch.from_numpy(X_test_scaled).float().reshape(([X_test_scaled.shape[0],w_size]))\n",
    "                            ) , batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "                        model = UsadModel(w_size, z_size)\n",
    "                        model.load_state_dict(torch.load(f'./../MODEL_Saved/USAD/MODEL{WINDOW_SIZE}/model{iter}'))\n",
    "                        model.eval()\n",
    "\n",
    "\n",
    "                        results=testing(model,test_loader)\n",
    "                        y_pred=np.concatenate([torch.stack(results[:-1]).flatten().detach().cpu().numpy(),\n",
    "                                            results[-1].flatten().detach().cpu().numpy()])\n",
    "                        \n",
    "                        results=testing(model,val_loader)\n",
    "                        y_val=np.concatenate([torch.stack(results[:-1]).flatten().detach().cpu().numpy(),\n",
    "                                            results[-1].flatten().detach().cpu().numpy()])\n",
    "\n",
    "                        score=y_val\n",
    "\n",
    "                        if(k==0):\n",
    "                            q1, q3 = np.percentile(score, 25), np.percentile(score, 75)\n",
    "                            iqr = q3 - q1\n",
    "                            th = q3 + t* iqr\n",
    "                        if(k==1):\n",
    "                            median = np.median(score)\n",
    "                            mad = 1.4826 * np.median(np.abs(score - median))\n",
    "                            th = median + t * mad\n",
    "\n",
    "                        if(k==2):\n",
    "                            mean, std = np.mean(score), np.std(score)\n",
    "                            th = mean + t * std\n",
    "\n",
    "\n",
    "                        score=y_pred\n",
    "                        score=score>th\n",
    "                        true_= df['anomaly'][WINDOW_SIZE:]>=1\n",
    "                        prediction_ = score[:].astype(int)==1\n",
    "\n",
    "                        TP = TP+(true_ & prediction_).sum()   \n",
    "                        TN = TN+(~true_ & ~prediction_).sum()  \n",
    "                        FP = FP+(~true_ & prediction_).sum()    \n",
    "                        FN = FN+(true_ & ~prediction_).sum()    \n",
    "                        iter=iter+1\n",
    "                      \n",
    "            PREC=TP / (TP + FP)\n",
    "            REC = TP/ (TP+FN)\n",
    "            f1=2 * PREC * REC/(PREC + REC)\n",
    "            m=['IQR','MAD','STD']\n",
    "            row=[t,mt[k],f1,PREC,REC,TP,TN,FP,FN]\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7732ff24e494917fdc04ba71f0b346f07dc6128216c19827ae3641c937bc9395"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
